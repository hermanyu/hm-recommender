{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d307d22-5fbe-4fb4-9756-1e809c32c660",
   "metadata": {},
   "source": [
    "# Framing The Correct Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0699cf5-d97b-4afe-a910-f5f33a29a554",
   "metadata": {},
   "source": [
    "Before we proceed with any coding or modeling, it is important to specify the exact nature of what we are trying to accomplish.\n",
    "\n",
    "The ultimate goal of this project is create a recommender system based on <a href='https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations'> H&M's online sales data </a> from 2018 to 2020. The data set comprises of:\n",
    "1. A log of every online transaction by user id and product id over the 2 year span. Contains 31,788,324 transactions in total.\n",
    "2. Meta data on each product by product id, including apparel type, department, etc.\n",
    "3. An image corresponding to products by product id. Note that some images may be corrupted, unintelligible, or straight-up missing.\n",
    "\n",
    "Firstly, we must decide on what exactly our recommender system is trying to predict. At its core, a recommender system takes pre-recorded interactions between users and items and tries to predict future/unknown interactions between specific users with specific items. \n",
    "\n",
    "Notice that we have transaction data but not rating data. This means each user's interaction with the product is binary variable: 0 for not purchased, 1 for purchased. The binary nature of this data leads to our first interesting theoretical problem.\n",
    "\n",
    "**Question)** If a user-item interaction value is 0 (not purchased) is it because:\n",
    "- the user does not know of the item?\n",
    "- the user did not like the item?\n",
    "\n",
    "In other words, one of the major theoretical challenges presented by this data is the ambiguity in the *reason why a user did not buy an item*. Is it because they just don't know about it? Or is it because they didn't actually like it?\n",
    "\n",
    "Notice this same issue does not appear in more classical recommender models such as movie recommendations. In classical data sets (like Movie Lens) there is a very clear demarkation between \"did not see\" vs \"did not like\". A rating of 5 means the user saw and liked the movie, a rating of 1 means the user saw and did not like the movie, and a missing value is interpreted as the user did not see the movie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55ab82-5b19-4d6c-bce0-437b8ea8389d",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "\n",
    "So what exactly do we want our recommender system to do. A naive answer might be the following:\n",
    "\n",
    "**Objective 1)** Given a user's past purchases, predict how likely a user would purchase a given item, if they were shown that item.\n",
    "\n",
    "However, if we ponder for a second how we would achieve this, we quickly realize a fundamental issue: in order for a model to correctly predict a probability, it needs to have access to negative cases. In our data set, we only have access to positive cases, which leads to an issue: the model can just cheat and guess \"users like everything and will buy anything with probability 1\". We have no way of penalizing such a guess because we don't have any clear examples of users NOT liking a product.\n",
    "\n",
    "One way to get around this roadblock is to artificially generate negative examples and this will require us to make assumptions on the nature of the data. The key issue is that we cannot distinguish between true negative interactions (users did not like the item) vs missing interactions (users did not see the item). So what if we made the simplifying assumption that all users must have seen a certain collection of items.\n",
    "\n",
    "More specifically, let's fix a time period, say Jan 2019 to Feb 2019. Let I(1), I(2), ..., I(10) denote the top 10 best sellers over this time period. Because these items were so popular, it seems reasonable to assume many people saw these items on the webpage during this time period. What if we made the following simplifying assumption?\n",
    "\n",
    "**Assumption)** all users on the webpage saw items I(1), ... , I(10) and missing interactions indicate the users saw but did not like the item enough to buy it.\n",
    "\n",
    "This now gives us a concrete set of negative examples which we can then contrast with positive examples (users buying an item) and we can then use this predict what a user will buy in the next time period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67972dcf-dfca-4f4f-8846-5af48b0703c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Of course, framing the problem as we did above comes with the caveat that we are making a rather heavy assumption on the visibility of products by popularity. Let us explore another objective that does not require such a heavy assumption. \n",
    "\n",
    "Instead of treating each user as a separate entity, what if we pooled all users together into a single entity, i.e. a \"general public\" or \"consumer\" entity. Instead of trying to recommend individual items to individual users, what if we instead tried to learn the trend of item sales over time. The idea is to build a model which can forecast the sales number of an individual item based entirely on the picture of the item.\n",
    "\n",
    "**Objective 2)** Use the image of each clothing item to forecast sales numbers over time.\n",
    "\n",
    "The idea is that in learning to make this forecast, the model will have to implicilty learn about seasonal trends and fashion/style trends across time. E.g. the model will have to recognize summer vs winter clothing, stylistic vs \"ugly\" clothing, etc. Once we know if an item will be a \"hot seller\" we can then make recommendations to all users. If the model can forecast sales numbers accurately, we also have the secondary benefit of being able to resolve supply-chain and production issues.\n",
    "\n",
    "This objective is much more well-founded in the sense that we do not need to make assumptions about the data. We also cut down on the \"size\" of the data set because we no longer need to distinguish individual users. Of course, this comes at the cost of no longer being able to make personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd038c0-3c82-47e1-a019-bc4bbe5ba57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import support_victor_machine as support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd9b705c-5a49-46cf-b2ac-32265fefa107",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv('../data/transactions_train.csv')\n",
    "\n",
    "articles = pd.read_csv('../data/articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c874a55-6458-493b-897f-7afb0b09bcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31788324, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffc41189-526e-49f6-8d71-25e816ecd3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105542"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04c0e0a5-8055-44ac-b7ee-d6caad18b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_img = []\n",
    "\n",
    "for i in articles['article_id']:\n",
    "    file = '0'+str(i)\n",
    "    folder = file[0:3]\n",
    "    try:\n",
    "        read_image('../resized_images/'+folder+'/'+file+'.jpg')\n",
    "        has_img.append(i)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a6fbcb3-4487-4ef1-9051-12b9a9fb0800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(has_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39b74537-f889-40f0-a649-05930159f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_with_img = transactions[ transactions['article_id'].isin(has_img) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a1674ba-63d1-4aab-b2a9-52b2a03575e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31651678, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_with_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32f0eca3-afcd-499e-9921-c8711dbcf5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_with_img.to_csv('../data/transactions_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b87c2-ad2c-446d-adea-f84210339a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
